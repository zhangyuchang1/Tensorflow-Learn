{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些参数\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.05\n",
    "MAX_STEP = 1000 # with this setting, it took less than 30 mins on my laptop to train. 10000\n",
    "TRIAN =True\n",
    "\n",
    "img_width = 32\n",
    "img_height = 32\n",
    "img_depth = 3\n",
    "img_pixel = img_width*img_height*img_pixel\n",
    "\n",
    "label_bytes = 1\n",
    "image_bytes = img_width*img_height*img_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 神经网络模型与猫狗大战一样\n",
    "\n",
    "def inference(images):\n",
    "    # vonv1 \n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = tf.get_variable('weights', \n",
    "                                  shape = [3, 3, 3, 96],\n",
    "                                  dtype = tf.float32, \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.05,dtype=tf.float32)\n",
    "                                  ) \n",
    "        biases = tf.get_variable('biases', \n",
    "                                 shape=[96],\n",
    "                                 dtype=tf.float32, \n",
    "                                 initializer=tf.constant_initializer(0.0)\n",
    "                                 )\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name= scope.name)\n",
    "    \n",
    "    \n",
    "    #pool1 and norm1\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],\n",
    "                               padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                               beta=0.75, name='norm1')\n",
    "\n",
    "    \n",
    "    conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3,3,96, 64],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.05,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[64], \n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "\n",
    "\n",
    "    #pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                          beta=0.75,name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],\n",
    "                               padding='SAME',name='pooling2')\n",
    "\n",
    "\n",
    "    #local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[BATCH_SIZE, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[dim,384],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.004,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[384],\n",
    "                                 dtype=tf.float32, \n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "\n",
    "    #local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[384, 192],\n",
    "                                  dtype =tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=float32)\n",
    "                                  )\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[192],\n",
    "                                 dtype=float32,\n",
    "                                 initializer=tf.constant_initializer(0.1)\n",
    "                                 )\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "\n",
    "     # softmax层\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.get_variable('soft_linear',\n",
    "                                  shape=[192, 10],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32)\n",
    "                                  )\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[10],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1)\n",
    "                                 )\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    "\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算损失\n",
    "def losses(logits,labels):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        # cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        #     logits=logits,\n",
    "        #     labels=labels,\n",
    "        #     name='xentropy_per_example'\n",
    "        # )\n",
    "        # 这种不用 onehot\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                  logits=logits,\n",
    "            labels=labels,\n",
    "            name='xentropy_per_example'\n",
    "        )\n",
    "        \n",
    "\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name + '/loss',loss)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train():\n",
    "    my_global_step = tf.Variable(name='global_step',trainable=False)\n",
    "    \n",
    "    data_dir = 'data/'\n",
    "    log_dir = 'log/'\n",
    "    \n",
    "    images, lables = cifar10_input.read_cifar10(data_dir=data_dir,\n",
    "                                                is_train=True,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                is_suffle=True)\n",
    "    logits = inference(images)\n",
    "    loss = losses(logits, lables)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=my_global_step)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    try:\n",
    "        for step in np.arange(MAX_STEP):\n",
    "            if coord.should_stop():\n",
    "                break\n",
    "            _, loss_value = sess.run(train_op, loss)\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('step %d, loss %.4f' %(step, loss_value))\n",
    "            \n",
    "            if sep % 100 == 0:\n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            if step % 2000 == 0 or (step + 1) == MAX_STEP:\n",
    "                checkpoint_path = os.path.join(log_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "                \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        \n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "initial_value must be specified.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-5685874baa98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmy_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\u001b[0m\n\u001b[1;32m    197\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m           expected_shape=expected_shape)\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initial_value must be specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0minit_from_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: initial_value must be specified."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}